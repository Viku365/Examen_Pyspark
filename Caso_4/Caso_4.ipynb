{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6faa8f82-8459-4b9d-b8be-b985c9f761af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza de datos con PySpark: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffad86de-2091-43f9-97e4-c94456fee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los [datos](https://tajamar365.sharepoint.com/:x:/s/3405-MasterIA2024-2025/ETYTQ0c-i6FLjM8rZ4iT1cgB6ipFAkainM-4V9M8DXsBiA?e=PeMtvh) fueron extraídos (scrapeados) del sitio web de Glassdoor y recoge los salarios de distintos puestos relacionados a Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4faaff2f-8a8b-4f78-9b5f-15f6f486f4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .option(\"multiline\", \"true\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .csv(\"/FileStore/Examen/ds_jobs.csv\")\n",
    "# Mostrar los datos bien estructurados\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd64de3-16db-4ce1-89ee-026c842262a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolver los siguientes requerimientos, para cada operación/moficación imprima como van quedadndo los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e55e35-5fe8-4a9a-a14a-d8d15dbe45b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Cargar los datos y mostrar el esquema o la informacion de las columnas y el tip de dato de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22efee7-398f-4d61-afcd-b11aa436494c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el esquema detallado\n",
    "print(\"Esquema del DataFrame:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Mostrar información de las columnas y tipos de datos\n",
    "print(\"Columnas y tipos de datos:\")\n",
    "for field in df.schema.fields:\n",
    "    print(f\"{field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4b636b-5e44-4afa-9d44-0666b991cec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4dda196-fe87-429c-a872-10640e28dd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar duplicados basados en todas las columnas\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "# Calcular los valores eliminados\n",
    "df_duplicates_removed = df.subtract(df_no_duplicates)\n",
    "\n",
    "# Mostrar los valores eliminados\n",
    "print(f\"Total de filas eliminadas: {df_duplicates_removed.count()}\")\n",
    "display(df_duplicates_removed)\n",
    "\n",
    "# Mostrar los datos sin duplicados\n",
    "display(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b97746-8b29-4550-9e1b-be2876c2a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Decidir que hacer con los datos faltantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4d5d9c-6be9-4f38-9e63-3e92662baab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lo mejor seria eliminar esos datos ya que perderia el valor al completo si nos inventamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaedda51-4a41-41cd-aba7-75f0e5a3c5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Decidir que hacer con los valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "504c2b33-5665-4f9f-bf66-90f52db81cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Al no tener muchos valores nulos lo mejor seria rellenarlos con datos promedios de la empresa segun antigüedad habria que juzgar cada dato para darle un valor apropiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a52cdf-8904-4189-894c-3ec9222aace7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "\n",
    "# Filtrar filas con valores nulos en cualquier columna\n",
    "df_null_values = df_no_duplicates.filter(\n",
    "    reduce(\n",
    "        lambda x, y: x | y,\n",
    "        [col(c).isNull() for c in df_no_duplicates.columns]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar el número de filas con valores nulos\n",
    "print(f\"Total de filas con valores nulos: {df_null_values.count()}\")\n",
    "\n",
    "# Mostrar las filas con valores nulos\n",
    "display(df_null_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb3b1ca-24ac-4206-8906-3ecee5d6c5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuántos registros tiene el csv?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed16633f-1554-4ae2-91a3-bc259a8003e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar el número de registros\n",
    "total_registros = df_no_duplicates.count()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"Total de registros en el CSV: {total_registros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9aac77-6fe5-4a54-b6ae-c152feee149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Mostrar los valores únicos de `Job title` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b37aaeb-f1c8-4d95-aa6f-f2812cbb7f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna Job Title\n",
    "unique_job_titles = df.select(\"Job Title\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "print(f\"Total de valores únicos en 'Job Title': {unique_job_titles.count()}\")\n",
    "display(unique_job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df06a700-7dd6-44c8-877c-52819ffb73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Remover la letra `K` de la columna `Salary Estimate` y multiplicar por 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2987e4-4dd9-4f4d-b51b-b18a5f55b3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, split, col\n",
    "\n",
    "# Limpiar la columna Salary Estimate (eliminar '$' y 'K', multiplicar por 1000)\n",
    "df_transformed = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"\\$\", \"\")  # Eliminar el símbolo '$'\n",
    ")\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"K\", \"\")  # Eliminar la letra 'K'\n",
    ")\n",
    "\n",
    "# Extraer el salario mínimo y máximo y multiplicar por 1000 si es necesario\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"Salary Min\",\n",
    "    (split(col(\"Salary Estimate\"), \"-\")[0].cast(\"double\") * 1000)  # Multiplicar por 1000\n",
    ").withColumn(\n",
    "    \"Salary Max\",\n",
    "    (split(col(\"Salary Estimate\"), \"-\")[1].cast(\"double\") * 1000)  # Multiplicar por 1000\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas Salary Min y Salary Max\n",
    "display(df_transformed.select(\"Salary Estimate\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f2bbcd-5c78-4d10-9118-e105f611b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Mostrar los valores únicos del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8674dc6a-3961-4cdf-9a0f-d6057104da88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna Salary Estimate\n",
    "unique_salary_estimates = df.select(\"Salary Estimate\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_salary_estimates.count()\n",
    "print(f\"Total de valores únicos en 'Salary Estimate': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_salary_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5d10ab-fd9e-4d8b-bf7f-f8d25b9ed1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Eliminar `(Glassdoor est.)` y `(Employer est.)` del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fea457-a050-4288-bc2a-ee91b1917465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim\n",
    "\n",
    "# Eliminar \"(Glassdoor est.)\" y \"(Employer est.)\" del campo Salary Estimate\n",
    "df_cleaned = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    trim(regexp_replace(col(\"Salary Estimate\"), r\"\\s*\\(Glassdoor est\\.|\\(Employer est\\.\\)\", \"\"))\n",
    ")\n",
    "\n",
    "# Mostrar los datos limpios\n",
    "display(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ff37c4-17a0-4eb4-b170-a131bd9b7844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Mostrar de mayor a menor los valores del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0248dbdb-0422-4205-ae41-a2545829e13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, asc, desc\n",
    "\n",
    "# Extraer el valor máximo del rango salarial y convertirlo a numérico\n",
    "df_with_salary_numeric = df.withColumn(\n",
    "    \"Salary Max\",\n",
    "    (regexp_extract(col(\"Salary Estimate\"), r\"-\\$(\\d+)K\", 1).cast(\"double\") * 1000)\n",
    ")\n",
    "\n",
    "# Ordenar por el valor máximo del salario en orden descendente\n",
    "df_sorted = df_with_salary_numeric.orderBy(col(\"Salary Max\").desc())\n",
    "\n",
    "# Mostrar los datos ordenados\n",
    "display(df_sorted.select(\"Salary Estimate\", \"Salary Max\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20074ca-ff4d-45ba-a548-369bd3bdadc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. De la columna `Job Description` quitar los saltos de linea `\\n` del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5900e6c-0d0c-48cc-83f3-c5d2dcca5d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remover los saltos de línea (\\n) de la columna Job Description\n",
    "df_cleaned = df.withColumn(\n",
    "    \"Job Description\",\n",
    "    regexp_replace(col(\"Job Description\"), r\"[\\r\\n]+\", \" \")\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame limpio\n",
    "display(df_cleaned.select(\"Job Description\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b8b421-3881-4bae-95c0-b79a37422e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. De la columna `Rating` muestre los valores unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92aca518-7f79-4866-be9b-c5421bad5043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna Rating\n",
    "unique_ratings = df.select(\"Rating\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_ratings.count()\n",
    "print(f\"Total de valores únicos en 'Rating': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd85fcdd-3514-4ad2-ab52-d1f21b825f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Del campo `Rating` reemplazar los `-1.0` por `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8c6583-42fe-4831-88f3-59554d719409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Reemplazar los valores -1.0 por 0.0 en la columna Rating\n",
    "df_updated = df.withColumn(\n",
    "    \"Rating\",\n",
    "    when(col(\"Rating\") == -1.0, 0.0).otherwise(col(\"Rating\"))\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "display(df_updated.select(\"Rating\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd30cd0d-46a3-4f2f-84f8-4c7e81d7d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Mostrar los valores unicos y ordenar los valores del campo `Company Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c89e1a-d6c2-4fbe-ba1e-f775d2a7aff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Obtener los valores únicos y ordenarlos\n",
    "unique_company_names = df.select(\"Company Name\").distinct().orderBy(col(\"Company Name\").asc())\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_company_names.count()\n",
    "print(f\"Total de valores únicos en 'Company Name': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos y ordenados\n",
    "display(unique_company_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2e2a45-3570-49a6-823c-882f5714a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Quitar todos los caracteres innecesarios que encuentres en el campo `Company Name`. Por ejemplo los saltos de linea `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fb11d0-bcca-4cff-a671-d2f8a7a1b311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col\n",
    "\n",
    "# Limpiar la columna Company Name\n",
    "df_cleaned = df.withColumn(\n",
    "    \"Company Name\",\n",
    "    trim(regexp_replace(col(\"Company Name\"), r\"[\\r\\n\\t]\", \"\"))  # Elimina \\r, \\n y \\t\n",
    ")\n",
    "\n",
    "# Mostrar los datos limpios\n",
    "display(df_cleaned.select(\"Company Name\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e594ec2-2d3f-429a-8cf4-7a6815567aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. En el campo `Location` convertir esa columna en dos: `City` y `State`. Las ciudades que tengas en `Location` asignar a la columna `City`. Lo mismo para `State`. Luego elimine la columna `Location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a107128c-5ac3-4f6c-8b7f-94637edd3dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la columna Location en City y State\n",
    "df_transformed = df.withColumn(\"City\", split(col(\"Location\"), \", \")[0]) \\\n",
    "                   .withColumn(\"State\", split(col(\"Location\"), \", \")[1]) \\\n",
    "                   .drop(\"Location\")  # Eliminar la columna original Location\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas\n",
    "display(df_transformed.select(\"City\", \"State\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed408ad-803a-4209-9dae-57b7418724ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Repetir la misma lógica de la pregunta 16 pero para el campo `Headquarters`. En Headquarters dejar solo la ciudad, mientras que para el estado añadirla a una columna nueva ` Headquarter State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faab9989-9e3e-450f-878e-1a066177069b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la columna Headquarters en City y Headquarter State\n",
    "df_transformed = df.withColumn(\"City\", split(col(\"Headquarters\"), \", \")[0]) \\\n",
    "                   .withColumn(\"Headquarter State\", split(col(\"Headquarters\"), \", \")[1]) \\\n",
    "                   .drop(\"Headquarters\")  # Eliminar la columna original Headquarters\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas\n",
    "display(df_transformed.select(\"City\", \"Headquarter State\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c9f37c-dbe5-4f0e-8f9b-7c4b4cfe8ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Muestre los valores únicos del campo `Headquarter State` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5715a4-ae97-4365-8ce0-d53078e8ca09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la columna Headquarters en City y Headquarter State\n",
    "df_transformed = df.withColumn(\"City\", split(col(\"Headquarters\"), \", \")[0]) \\\n",
    "                   .withColumn(\"Headquarter State\", split(col(\"Headquarters\"), \", \")[1]) \\\n",
    "                   .drop(\"Headquarters\")  # Eliminar la columna original Headquarters\n",
    "\n",
    "# Obtener los valores únicos de la columna Headquarter State\n",
    "unique_headquarter_states = df_transformed.select(\"Headquarter State\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_headquarter_states.count()\n",
    "print(f\"Total de valores únicos en 'Headquarter State': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_headquarter_states.orderBy(col(\"Headquarter State\").asc()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7895f10-d9c8-443b-a225-848031030eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Mostrar valores unicos del campo `Size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1aad47a-050e-45e7-851a-d7589763e7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Obtener los valores únicos de la columna Size\n",
    "unique_sizes = df.select(\"Size\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_sizes.count()\n",
    "print(f\"Total de valores únicos en 'Size': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos ordenados alfabéticamente\n",
    "display(unique_sizes.orderBy(col(\"Size\").asc()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f6a6b8-52b5-4e2e-9850-6dadc97f206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Quitar 'employee' de los registros del campo `Size`. Elimine tambien otros caracteres basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588ba408-78bf-4b75-9d27-7b4538523afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col\n",
    "\n",
    "# Limpiar el campo Size eliminando 'employee' y otros caracteres basura\n",
    "df_cleaned = df.withColumn(\n",
    "    \"Size\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            col(\"Size\"), r\"(?i)employee|[^a-zA-Z0-9\\s\\-\\>]+\", \"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con los valores limpios\n",
    "display(df_cleaned.select(\"Size\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36dfe0a0-9a25-40bc-8282-cba498d2badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Reemplazar la palabra 'to' por '-' en todos los registros del campo `Size`. Reemplazar tambien '-1' por 'Unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f71ca6-92d6-4f00-89cb-4a5ac9b65fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar 'to' por '-' y '-1' por 'Unknown' en el campo Size\n",
    "df_transformed = df_cleaned.withColumn(\n",
    "    \"Size\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(col(\"Size\"), r\"\\bto\\b\", \"-\"),  # Reemplazar 'to' por '-'\n",
    "        r\"-1\", \"Unknown\"  # Reemplazar '-1' por 'Unknown'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame transformado\n",
    "display(df_transformed.select(\"Size\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f598578e-b86d-4de7-a298-709ec19648ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22. Mostrar el tipo de dato del campo `Type of ownership` y sus registros unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64425d03-57bd-45bf-bdf0-a2e8e5ce8197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Mostrar el tipo de dato de la columna\n",
    "type_of_ownership_dtype = df.schema[\"Type of ownership\"].dataType\n",
    "print(f\"Tipo de dato del campo 'Type of ownership': {type_of_ownership_dtype}\")\n",
    "\n",
    "# Obtener los registros únicos de la columna\n",
    "unique_type_of_ownership = df.select(\"Type of ownership\").distinct()\n",
    "\n",
    "# Contar el número de registros únicos\n",
    "total_unique = unique_type_of_ownership.count()\n",
    "print(f\"Total de registros únicos en 'Type of ownership': {total_unique}\")\n",
    "\n",
    "# Mostrar los registros únicos\n",
    "display(unique_type_of_ownership.orderBy(col(\"Type of ownership\").asc()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f42ef72-f068-4b46-b1d3-6a5d59290322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23. Cambiar '-1' por 'Unknown' en todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14398da2-c1f6-4023-abf9-a5cc111e9baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Reemplazar '-1' por 'Unknown' en la columna Type of ownership\n",
    "df_updated = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    when(col(\"Type of ownership\") == \"-1\", \"Unknown\").otherwise(col(\"Type of ownership\"))\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "display(df_updated.select(\"Type of ownership\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9f9087-9cce-4213-b99b-f573757dbd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24. Cambiar:  \n",
    "-  `Company - Public` por `Public Company`  \n",
    "-  `Company - Private` por `Private Company`  \n",
    "-  `Private Practice / Firm` por `Private Company`  \n",
    "-  `Subsidiary or Business Segment` por `Business`  \n",
    "-  `College / University` por `Education`  \n",
    "En todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170867d3-7b05-4f76-b9ae-8ab7df5c9087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Realizar los reemplazos en la columna Type of ownership\n",
    "df_updated = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    when(col(\"Type of ownership\") == \"Company - Public\", \"Public Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Company - Private\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Private Practice / Firm\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Subsidiary or Business Segment\", \"Business\")\n",
    "    .when(col(\"Type of ownership\") == \"College / University\", \"Education\")\n",
    "    .otherwise(col(\"Type of ownership\"))  # Mantener los demás valores igual\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con los valores actualizados\n",
    "display(df_updated.select(\"Type of ownership\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed329cf5-1daa-4dd9-9b16-d2ab23bbf486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25. Mostrar el tipo de dato y los valores unicos del campo `Industry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923392c2-37de-4631-ad59-568337ebd761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Mostrar el tipo de dato de la columna Industry\n",
    "industry_dtype = df.schema[\"Industry\"].dataType\n",
    "print(f\"Tipo de dato del campo 'Industry': {industry_dtype}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna Industry\n",
    "unique_industries = df.select(\"Industry\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_industries.count()\n",
    "print(f\"Total de valores únicos en 'Industry': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_industries.orderBy(col(\"Industry\").asc()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06a12fb-1a33-4cfa-aadb-c01e00a9c4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26. En el mismo campo de `Industry` reemplazar '-1' por 'Not Available' y '&' por 'and'.  Vuelva a imprimir los valores unicos en orden alfabético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1041b6-b2c8-4b96-9351-10f8ea2a4f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and' en la columna Industry\n",
    "df_updated = df.withColumn(\n",
    "    \"Industry\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(col(\"Industry\"), r\"-1\", \"Not Available\"),  # Reemplazar '-1' por 'Not Available'\n",
    "        r\"&\", \"and\"  # Reemplazar '&' por 'and'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Obtener los valores únicos de la columna Industry, ordenados alfabéticamente\n",
    "unique_industries = df_updated.select(\"Industry\").distinct().orderBy(col(\"Industry\").asc())\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_industries.count()\n",
    "print(f\"Total de valores únicos en 'Industry': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_industries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2490bb6-1f88-4410-a0c4-75826c48b70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27. Para el campo `Sector`, muestre el tipo de dato y los valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6345468-3d84-41bd-99b6-544a161b59c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Mostrar el tipo de dato de la columna Sector\n",
    "sector_dtype = df.schema[\"Sector\"].dataType\n",
    "print(f\"Tipo de dato del campo 'Sector': {sector_dtype}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna Sector\n",
    "unique_sectors = df.select(\"Sector\").distinct()\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_sectors.count()\n",
    "print(f\"Total de valores únicos en 'Sector': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_sectors.orderBy(col(\"Sector\").asc()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d92b2b-c4be-4aca-b734-3ef8d80b62cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28. Aplica la misma lógica de la pregunta 26 pero sobre el campo `Sector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29ce386-b29b-4aa6-9b7f-0fdbc7d36eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and' en la columna Sector\n",
    "df_transformed = df.withColumn(\n",
    "    \"Sector\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(col(\"Sector\"), r\"-1\", \"Not Available\"),  # Reemplazar '-1' por 'Not Available'\n",
    "        r\"&\", \"and\"  # Reemplazar '&' por 'and'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Obtener los valores únicos de la columna Sector, ordenados alfabéticamente\n",
    "unique_sectors = df_transformed.select(\"Sector\").distinct().orderBy(col(\"Sector\").asc())\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_sectors.count()\n",
    "print(f\"Total de valores únicos en 'Sector': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_sectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3770cb-694e-439f-8038-d28da4e3ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29. Para el campo `Revenue`, muestre el tipo de dato y los valores únicos en orden ascedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0845180f-be29-49c3-881c-b89fa4ffe33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Mostrar el tipo de dato de la columna Revenue\n",
    "revenue_dtype = df.schema[\"Revenue\"].dataType\n",
    "print(f\"Tipo de dato del campo 'Revenue': {revenue_dtype}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna Revenue\n",
    "unique_revenues = df.select(\"Revenue\").distinct().orderBy(col(\"Revenue\").asc())\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_revenues.count()\n",
    "print(f\"Total de valores únicos en 'Revenue': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_revenues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb56eb9-3684-49cd-b20f-fd368d23b499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30. En el campo `Revenue`, cambiar:  \n",
    "-  `-1` por `N/A`  \n",
    "-  `Unknown / Non-Applicable` por `N/A`  \n",
    "-  `Less than $1 million (USD)` por `Less than 1`\n",
    "-  Quitar `$` y `(USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c693e4-229e-434c-ad12-9ae94df4a3c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Realizar los reemplazos y limpiar la columna Revenue\n",
    "df_updated = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(col(\"Revenue\"), r\"-1\", \"N/A\"),  # Reemplazar '-1' por 'N/A'\n",
    "                r\"Unknown / Non-Applicable\", \"N/A\"  # Reemplazar 'Unknown / Non-Applicable' por 'N/A'\n",
    "            ),\n",
    "            r\"Less than \\$1 million \\(USD\\)\", \"Less than 1\"  # Reemplazar 'Less than $1 million (USD)' por 'Less than 1'\n",
    "        ),\n",
    "        r\"\\$|\\(USD\\)\", \"\"  # Quitar los símbolos '$' y '(USD)'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "display(df_updated.select(\"Revenue\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89472f2-855f-4edc-bc47-d2654069bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31. Borrar el campo `Competitors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e77f624-046c-4d13-9f4e-a04edfe0a1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar la columna Competitors\n",
    "df_without_competitors = df.drop(\"Competitors\")\n",
    "\n",
    "# Mostrar el DataFrame sin la columna Competitors\n",
    "display(df_without_competitors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d3b8e8-a020-4a19-9d85-fb3f635ed316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32. Crear tres columnas: `min_salary` (salario mínimo), `max_salary` (salario maximo) y `avg_salary` (salario promedio) a partir de los datos del campo `Salary Estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a24ba0-7c57-4a3b-be5e-d322fda3c64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, split, col\n",
    "\n",
    "# Paso 1: Eliminar el símbolo '$', el texto dentro de paréntesis y la letra 'K'\n",
    "df_cleaned = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"\\$\", \"\")  # Eliminar '$'\n",
    ")\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"\\(.*\\)\", \"\")  # Eliminar paréntesis y texto dentro\n",
    ")\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"K\", \"\")  # Eliminar 'K'\n",
    ")\n",
    "\n",
    "# Paso 2: Dividir la columna Salary Estimate en salario mínimo y máximo\n",
    "df_transformed = df_cleaned.withColumn(\n",
    "    \"min_salary\",\n",
    "    split(col(\"Salary Estimate\"), \"-\")[0].cast(\"double\")  # Obtener el salario mínimo (antes del '-')\n",
    ").withColumn(\n",
    "    \"max_salary\",\n",
    "    split(col(\"Salary Estimate\"), \"-\")[1].cast(\"double\")  # Obtener el salario máximo (después del '-')\n",
    ")\n",
    "\n",
    "# Paso 3: Crear la columna avg_salary como el promedio entre min_salary y max_salary\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"avg_salary\",\n",
    "    (col(\"min_salary\") + col(\"max_salary\")) / 2\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas\n",
    "display(df_transformed.select(\"Salary Estimate\", \"min_salary\", \"max_salary\", \"avg_salary\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359b377b-30ee-4c05-b7cd-f0f217a4d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33. Mostrar los valores unicos del campo `Founded` y el tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ebf2f2-9c5b-492e-a0d0-d1900be35f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Mostrar el tipo de dato de la columna Founded\n",
    "founded_dtype = df.schema[\"Founded\"].dataType\n",
    "print(f\"Tipo de dato del campo 'Founded': {founded_dtype}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna Founded, ordenados ascendentemente\n",
    "unique_founded = df.select(\"Founded\").distinct().orderBy(col(\"Founded\").asc())\n",
    "\n",
    "# Contar el número de valores únicos\n",
    "total_unique = unique_founded.count()\n",
    "print(f\"Total de valores únicos en 'Founded': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_founded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd2dd8d-9e02-46e5-b441-0b93d2775f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34. Reemplazar '-1' por '2024' en todos los registros del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1335122f-359b-44c6-82b2-1c95bab77df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Reemplazar '-1' por '2024' en la columna Founded\n",
    "df_updated = df.withColumn(\n",
    "    \"Founded\",\n",
    "    when(col(\"Founded\") == -1, 2024).otherwise(col(\"Founded\"))  # Reemplazar '-1' por 2024\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "display(df_updated.select(\"Founded\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e823336d-0258-4af5-8d13-c9a358b9295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35. Crear una nueva columna o campo que se llame `company_age` con los datos que se deducen del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2af0ba-70d9-4b0a-b745-93f548500b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, year, col\n",
    "\n",
    "# Obtener el año actual\n",
    "current_year = year(current_date())\n",
    "\n",
    "# Crear una nueva columna company_age basada en la columna Founded\n",
    "df_transformed = df.withColumn(\n",
    "    \"company_age\",\n",
    "    current_year - col(\"Founded\")  # Restar el año actual con el valor de Founded\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con la nueva columna company_age\n",
    "display(df_transformed.select(\"Founded\", \"company_age\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a8f7ed-434c-4ad1-ba31-eeecd65c117c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36. Crear una columna o campo que se llame: `Job Type` y en cada registro debe ir Senior, Junior o NA según los datos del campo `Job Title`.  \n",
    "- Cambiar 'sr' o 'senior' o 'lead' o 'principal' por `Senior` en el campo `Job Type`. No olvidar las mayúsculas.\n",
    "- Cambiar 'jr' o 'jr.' o cualquier otra variante por `Junior`.  \n",
    "- En cualquier otro caso distinto a los anteriores añadir NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243d1ee4-4bf1-46b1-b2fa-9ab032b47bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Crear la columna Job Type basada en las condiciones del campo Job Title\n",
    "df_transformed = df.withColumn(\n",
    "    \"Job Type\",\n",
    "    when(col(\"Job Title\").rlike(\"(?i)sr|senior|lead|principal\"), \"Senior\")  # Coincidir con 'sr', 'senior', 'lead', 'principal'\n",
    "    .when(col(\"Job Title\").rlike(\"(?i)jr|jr\\\\.\"), \"Junior\")  # Coincidir con 'jr', 'jr.', y otras variantes\n",
    "    .otherwise(\"NA\")  # En cualquier otro caso asignar 'NA'\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame con la nueva columna Job Type\n",
    "display(df_transformed.select(\"Job Title\", \"Job Type\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c70227b-c16c-4919-9aa2-9bf9514b76dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37. Muestra los registros únicos del campo `Job Type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1658438-75f1-495e-b022-94935dc0319f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los registros únicos del campo Job Type\n",
    "unique_job_types = df_transformed.select(\"Job Type\").distinct()\n",
    "\n",
    "# Contar el número de registros únicos\n",
    "total_unique = unique_job_types.count()\n",
    "print(f\"Total de valores únicos en 'Job Type': {total_unique}\")\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "display(unique_job_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd4dbad-048d-461f-bf2a-f54c557d16f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38. Partiendo del campo `Job Description` se extraer todas o las principales skills solicitadas por las empresas, por ejemplo: Python, Spark , Big Data. Cada Skill debe ir en una nueva columna de tipo Binaria ( 0 , 1) o Booleana (True,  False) de modo que cada skill va ser una nueva columna y si esa skill es solicitada por la empresa colocar 1 sino colocar 0. Por ejemplo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efeeba4-5536-4d3a-9726-8740822de8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Definir las skills a buscar en las descripciones de trabajo\n",
    "skills = [\"Python\", \"Spark\", \"Big Data\", \"Excel\", \"Hadoop\", \"Machine Learning\", \"SQL\", \"R\", \"Data Science\", \"Deep Learning\"]\n",
    "\n",
    "# Crear nuevas columnas para cada skill\n",
    "df_transformed = df\n",
    "for skill in skills:\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        skill, \n",
    "        when(col(\"Job Description\").rlike(f\"(?i){skill}\"), 1).otherwise(0)  # 1 si la skill está presente, 0 si no lo está\n",
    "    )\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas de skills\n",
    "display(df_transformed.select(\"Job Title\", *skills))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e86af1-e86e-48db-b9d8-832cee782b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por ejemplo:  \n",
    "| Job Title         | Salary Estimate | Job Description                                 | Rating | Company Name       | Size       | Founded | Type of ownership         | Industry                       | Sector                         | Same State      | company_age | Python | Excel |\n",
    "|--------------------|-----------------|-------------------------------------------------|--------|--------------------|------------|---------|---------------------------|--------------------------------|--------------------------------|----------------|-------------|--------|-------|\n",
    "| Sr Data Scientist | 137000-171000   | Description The Senior Data Scientist is resp... | 3.1    | Healthfirst        | 1001-5000  | 1993    | Nonprofit Organization    | Insurance Carriers            | Insurance Carriers            | Same State      | 31          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Secure our Nation, Ignite your Future Join th... | 4.2    | ManTech            | 5001-10000 | 1968    | Public Company            | Research and Development      | Research and Development      | Same State      | 56          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Overview Analysis Group is one of the larges... | 3.8    | Analysis Group      | 1001-5000  | 1981    | Private Company           | Consulting                    | Consulting                    | Same State      | 43          | 1      | 1     |\n",
    "| Data Scientist    | 137000-171000   | JOB DESCRIPTION: Do you have a passion for Da... | 3.5    | INFICON            | 501-1000   | 2000    | Public Company            | Electrical and Electronic Manufacturing | Electrical and Electronic Manufacturing | Different State | 24          | 1      | 1     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7792213-00dc-4928-95b9-9b579ec04e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Definir las skills a buscar en las descripciones de trabajo\n",
    "skills = [\"Python\", \"Spark\", \"Big Data\", \"Excel\", \"Hadoop\", \"Machine Learning\", \"SQL\", \"R\", \"Data Science\", \"Deep Learning\"]\n",
    "\n",
    "# Crear nuevas columnas para cada skill\n",
    "df_transformed = df\n",
    "for skill in skills:\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        skill, \n",
    "        when(col(\"Job Description\").rlike(f\"(?i){skill}\"), 1).otherwise(0)  # 1 si la skill está presente, 0 si no lo está\n",
    "    )\n",
    "\n",
    "# Mostrar el DataFrame con las nuevas columnas de skills\n",
    "display(df_transformed.select(\"Job Title\", *skills))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5609a545-1510-4707-a0bd-a937e37675a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39. Exportar dataset final a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725088a8-2138-4591-88a2-a61b2a9392c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fe24a6-5fc3-4ec4-9b76-56fd2b3e36ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40. Extraer todos los insights posibles que sean de valor o utilidad. Cree nuevas columnas, agrupar,  filtrar hacer varios plots que muestren dichos insights que sean de utilidad para una empresa o para un usuario. Elabore conclusiones con los insights encontrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47336ad8-9ded-4bf1-bb31-77bee1ff1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
